{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/icds/RISE/sw8/anaconda/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#What I’m doing here is basically importing all the PySpark components I need for this project.\n",
    "#I load Spark SQL, ML, and some feature engineering tools, plus the classifier and evaluator.\n",
    "#This is just the standard setup before I can run any Spark pipeline.\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/storage/home/yfl5682/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/yfl5682/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/01 13:01:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Here I’m just creating the Spark session\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"DS410_TimeSeries_Tweets\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Disable ANSI mode so malformed values become NULL instead of killing the job\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the file we need for the pipeline and ml\n",
    "\n",
    "spx_path = \"/storage/work/yfl5682/Project/SP500/SPX_full_5min_with_datetime_parts.csv\"\n",
    "tweets_path = \"/storage/work/yfl5682/Project/tweets_with_topics_v2_flat.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPX rows: 357535\n",
      "+-------------------+-------+-------+-------+-------+\n",
      "|bar_time           |Open   |High   |Low    |Close  |\n",
      "+-------------------+-------+-------+-------+-------+\n",
      "|2008-01-02 09:30:00|1467.97|1470.14|1467.97|1470.05|\n",
      "|2008-01-02 09:35:00|1470.17|1470.17|1467.88|1469.49|\n",
      "|2008-01-02 09:40:00|1469.78|1471.71|1469.39|1471.22|\n",
      "|2008-01-02 09:45:00|1471.56|1471.77|1470.69|1470.78|\n",
      "|2008-01-02 09:50:00|1470.28|1471.06|1470.1 |1470.74|\n",
      "+-------------------+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------------------+--------------------+\n",
      "|           bar_time|  Close|          rv_pre_30m|         rv_post_30m|\n",
      "+-------------------+-------+--------------------+--------------------+\n",
      "|2008-01-02 09:30:00|1470.05|                NULL|0.004660684218949346|\n",
      "|2008-01-02 09:35:00|1469.49|                NULL|0.004714627426470183|\n",
      "|2008-01-02 09:40:00|1471.22|3.810119996833424E-4|0.004947337820406521|\n",
      "|2008-01-02 09:45:00|1470.78|0.001236740274360...|0.004863112334274586|\n",
      "|2008-01-02 09:50:00|1470.74|0.001272398144075946|0.004897381750463964|\n",
      "+-------------------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# PART A. Load SPX data, build bar_time, compute RV\n",
    "# ==================================================\n",
    "\n",
    "# In this cell what I’m basically doing is preparing the SPX 5-minute time series\n",
    "# and building the volatility labels I’ll later join with Trump tweets.\n",
    "\n",
    "# I read in the SPX CSV that already has year/month/day/hour/minute/second\n",
    "# split out as separate columns. Then I cast all of these time parts and the OHLC(Open, High, Low, Close), check SP 500 file \n",
    "# price fields into the correct numeric types so Spark can do proper math on them.\n",
    "\n",
    "# I then reconstruct a full timestamp column called bar_time by stitching\n",
    "# together the year/month/day/hour/minute/second into a single string and then\n",
    "# converting it to a real Spark timestamp. This gives me a clean time index for\n",
    "# each 5-minute bar. if you guys want to use 1 minute bar it will be still very simple. \n",
    "\n",
    "# Once I have bar_time, I compute log returns of the SPX close price\n",
    "# within each trading day using a window ordered by time. then I build\n",
    "# realized volatility features, 30-minute pre-event RV, using the previous 6 bars;\n",
    "# and 30-minute post-event RV, using the current bar plus the next 6 bars.\n",
    "# These two columns: `rv_pre_30m` and `rv_post_30m`, will later serve as my\n",
    "# “before vs after tweet” volatility measures.\n",
    "\n",
    "\n",
    "\n",
    "# 2.1 Read SPX CSV (5-min bars with datetime parts)\n",
    "spx_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(spx_path)\n",
    ")\n",
    "\n",
    "# 2.2 Cast numeric fields to proper types\n",
    "spx = (\n",
    "    spx_raw\n",
    "    .withColumn(\"year\",   F.col(\"year\").cast(T.IntegerType()))\n",
    "    .withColumn(\"month\",  F.col(\"month\").cast(T.IntegerType()))\n",
    "    .withColumn(\"day\",    F.col(\"day\").cast(T.IntegerType()))\n",
    "    .withColumn(\"hour\",   F.col(\"hour\").cast(T.IntegerType()))\n",
    "    .withColumn(\"minute\", F.col(\"minute\").cast(T.IntegerType()))\n",
    "    .withColumn(\"second\", F.col(\"second\").cast(T.IntegerType()))\n",
    "    .withColumn(\"Open\",   F.col(\"Open\").cast(T.DoubleType()))\n",
    "    .withColumn(\"High\",   F.col(\"High\").cast(T.DoubleType()))\n",
    "    .withColumn(\"Low\",    F.col(\"Low\").cast(T.DoubleType()))\n",
    "    .withColumn(\"Close\",  F.col(\"Close\").cast(T.DoubleType()))\n",
    ")\n",
    "\n",
    "# 2.3 Reconstruct full timestamp for each bar\n",
    "spx_time_str = F.format_string(\n",
    "    \"%04d-%02d-%02d %02d:%02d:%02d\",\n",
    "    F.col(\"year\"), F.col(\"month\"), F.col(\"day\"),\n",
    "    F.col(\"hour\"), F.col(\"minute\"), F.col(\"second\")\n",
    ")\n",
    "\n",
    "spx = spx.withColumn(\n",
    "    \"bar_time\",\n",
    "    F.to_timestamp(spx_time_str, \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "print(\"SPX rows:\", spx.count())\n",
    "spx.select(\"bar_time\", \"Open\", \"High\", \"Low\", \"Close\").show(5, truncate=False)\n",
    "\n",
    "# 2.4 Compute log returns and 30-min pre/post realized volatility\n",
    "# Use a per-day window to avoid unnecessary full-table windows\n",
    "w_order = Window.partitionBy(F.to_date(\"bar_time\")).orderBy(\"bar_time\")\n",
    "\n",
    "spx_lr = spx.withColumn(\n",
    "    \"log_return\",\n",
    "    F.log(F.col(\"Close\") / F.lag(\"Close\").over(w_order))\n",
    ")\n",
    "\n",
    "# 6 previous / next bars = 30 minutes for 5-min data\n",
    "w_pre = w_order.rowsBetween(-6, -1)\n",
    "w_post = w_order.rowsBetween(0, 6)\n",
    "\n",
    "spx_rv = (\n",
    "    spx_lr\n",
    "    .withColumn(\n",
    "        \"rv_pre_30m\",\n",
    "        F.sqrt(F.sum(F.pow(F.col(\"log_return\"), F.lit(2.0))).over(w_pre))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"rv_post_30m\",\n",
    "        F.sqrt(F.sum(F.pow(F.col(\"log_return\"), F.lit(2.0))).over(w_post))\n",
    "    )\n",
    ")\n",
    "\n",
    "spx_rv.select(\"bar_time\", \"Close\", \"rv_pre_30m\", \"rv_post_30m\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------------------------------+-----------------------------------------------------------------+---------+---------+--------------------+---------+--------+--------+----------+\n",
      "|                 id|         tweet_time|                               category|                                                    blue_category|sentiment|intensity|during_trading_hours|favorites|retweets|text_len|num_exclam|\n",
      "+-------------------+-------------------+---------------------------------------+-----------------------------------------------------------------+---------+---------+--------------------+---------+--------+--------+----------+\n",
      "|  98454970654916608|2011-08-02 18:07:48|     Macroeconomics & Monetary Policies|                                          Market / Economy / Jobs| Negative|   Medium|               False|       49|     255|      66|         0|\n",
      "|1234653427789070336|2020-03-03 01:34:50|   Campaign / Rally / Election Politics|             Campaign / Elections / Rallies / Political Messaging| Positive|     High|               False|    73748|   17404|     255|         3|\n",
      "|1304875170860015617|2020-09-12 20:10:58|   Campaign / Rally / Election Politics|             Campaign / Elections / Rallies / Political Messaging| Negative|     High|               False|    80527|   23502|     289|         1|\n",
      "|1223640662689689602|2020-02-01 16:14:02|Personal, Social, or Non-Policy Content|Personal / Social / Non-Policy Content (Congrats, Holidays, Misc)| Positive|      Low|               False|   285863|   30209|      39|         1|\n",
      "|1215247978966986752|2020-01-09 12:24:31|Personal, Social, or Non-Policy Content|Personal / Social / Non-Policy Content (Congrats, Holidays, Misc)| Positive|      Low|                True|    48510|   11608|      16|         1|\n",
      "+-------------------+-------------------+---------------------------------------+-----------------------------------------------------------------+---------+---------+--------------------+---------+--------+--------+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# PART B. Load tweets, build tweet_time, basic features\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "\n",
    "# here what im trying to do is to rebuild a clean timestamp for each tweet, and engineer a few simple\n",
    "# features that I might use later in the ML pipeline.\n",
    "\n",
    "# I cast all the time-related columns plus favorites and retweets\n",
    "# into integers, and I drop any rows where the datetime pieces are incomplete.\n",
    "# I don’t want broken timestamps to mess up the time alignment with SPX.\n",
    "#\n",
    "# I reconstruct a full `tweet_time` timestamp column, similar to what\n",
    "# I did for SPX, by formatting year/month/day/hour/minute/second into a\n",
    "# single string and converting it to a Spark timestamp.\n",
    "#\n",
    "# I clean up the boolean flags `isRetweet` and `isDeleted` by turning\n",
    "# the original \"t\"/\"f\" strings into real Boolean columns. I’m not necessarily\n",
    "# using them in the main ML model yet, but I keep them in a clean format. Highly doubt that if this is even useful lmao...\n",
    "#\n",
    "# I build some super simple text-based features: length of the tweet\n",
    "# (`text_len`) and how many exclamation marks it has (`num_exclam`). At the\n",
    "# end I preview the key columns I care about: timing, topic labels, sentiment,\n",
    "# trading-hours flag, engagement metrics, and the basic text features. you guys could add more interesting features if u want. \n",
    "\n",
    "\n",
    "# 3.1 Read flat tweets CSV\n",
    "tweets_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(tweets_path)\n",
    ")\n",
    "\n",
    "# 3.2 Cast integer columns\n",
    "int_cols = [\n",
    "    \"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\",\n",
    "    \"favorites\", \"retweets\"\n",
    "]\n",
    "\n",
    "tweets_num = tweets_raw\n",
    "for c in int_cols:\n",
    "    tweets_num = tweets_num.withColumn(c, F.col(c).cast(T.IntegerType()))\n",
    "\n",
    "# Drop rows with incomplete datetime fields\n",
    "tweets_num = tweets_num.filter(\n",
    "    F.col(\"year\").isNotNull() &\n",
    "    F.col(\"month\").isNotNull() &\n",
    "    F.col(\"day\").isNotNull() &\n",
    "    F.col(\"hour\").isNotNull() &\n",
    "    F.col(\"minute\").isNotNull() &\n",
    "    F.col(\"second\").isNotNull()\n",
    ")\n",
    "\n",
    "# 3.3 Build full tweet timestamp\n",
    "tweet_time_str = F.format_string(\n",
    "    \"%04d-%02d-%02d %02d:%02d:%02d\",\n",
    "    F.col(\"year\"), F.col(\"month\"), F.col(\"day\"),\n",
    "    F.col(\"hour\"), F.col(\"minute\"), F.col(\"second\")\n",
    ")\n",
    "\n",
    "tweets = tweets_num.withColumn(\n",
    "    \"tweet_time\",\n",
    "    F.to_timestamp(tweet_time_str, \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "# 3.4 Boolean flags (we do not use them in ML, but keep them clean)\n",
    "tweets = (\n",
    "    tweets\n",
    "    .withColumn(\"isRetweet\", (F.col(\"isRetweet\") == F.lit(\"t\")).cast(T.BooleanType()))\n",
    "    .withColumn(\"isDeleted\", (F.col(\"isDeleted\") == F.lit(\"t\")).cast(T.BooleanType()))\n",
    ")\n",
    "\n",
    "# 3.5 Simple text features\n",
    "tweets = (\n",
    "    tweets\n",
    "    .withColumn(\"text_len\",   F.length(\"text\"))\n",
    "    .withColumn(\"num_exclam\", F.size(F.split(F.col(\"text\"), \"!\")) - F.lit(1))\n",
    ")\n",
    "\n",
    "tweets.select(\n",
    "    \"id\", \"tweet_time\", \"category\", \"blue_category\",\n",
    "    \"sentiment\", \"intensity\", \"during_trading_hours\",\n",
    "    \"favorites\", \"retweets\", \"text_len\", \"num_exclam\"\n",
    ").show(5, truncate=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events rows (tweets used in pipeline): 10870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-------+---------------------+---------------------+----------+----------+--------+--------+\n",
      "|id                 |tweet_time         |event_time         |Close  |rv_pre_30m           |rv_post_30m          |tweet_year|tweet_hour|spx_year|spx_hour|\n",
      "+-------------------+-------------------+-------------------+-------+---------------------+---------------------+----------+----------+--------+--------+\n",
      "|1001404640796336128|2018-05-29 10:07:26|2018-05-29 10:05:00|2709.29|0.0019230080247938965|0.0018386673426389981|2018      |10        |2018    |10      |\n",
      "|1001410457092218880|2018-05-29 10:30:32|2018-05-29 10:30:00|2704.19|0.0012550933708381099|0.0023081077350491125|2018      |10        |2018    |10      |\n",
      "|1001415199516254208|2018-05-29 10:49:23|2018-05-29 10:50:00|2705.22|0.001656133860550514 |0.0022112435484160416|2018      |10        |2018    |10      |\n",
      "|1001417880116891650|2018-05-29 11:00:02|2018-05-29 11:00:00|2698.87|0.0019272309432976154|0.0020400066064023476|2018      |11        |2018    |11      |\n",
      "|1001420270094168064|2018-05-29 11:09:32|2018-05-29 11:10:00|2701.13|0.0017407643404526263|0.0031069847934639756|2018      |11        |2018    |11      |\n",
      "|1001424695126880258|2018-05-29 11:27:07|2018-05-29 11:25:00|2700.56|0.002210637898972629 |0.002807087341143202 |2018      |11        |2018    |11      |\n",
      "|1001455721588969472|2018-05-29 13:30:24|2018-05-29 13:30:00|2682.05|0.0018390545052083933|0.003071718135565342 |2018      |13        |2018    |13      |\n",
      "|1001807174249713664|2018-05-30 12:46:57|2018-05-30 12:45:00|2721.15|8.362739852883948E-4 |0.0013075401579683982|2018      |12        |2018    |12      |\n",
      "|1001807204519997442|2018-05-30 12:47:04|2018-05-30 12:45:00|2721.15|8.362739852883948E-4 |0.0013075401579683982|2018      |12        |2018    |12      |\n",
      "|1001807216297627648|2018-05-30 12:47:07|2018-05-30 12:45:00|2721.15|8.362739852883948E-4 |0.0013075401579683982|2018      |12        |2018    |12      |\n",
      "+-------------------+-------------------+-------------------+-------+---------------------+---------------------+----------+----------+--------+--------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# PART C. Match tweets to nearest SPX bar within ±10 minutes\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# In this cell what I’m basically doing is aligning each tweet with the closest\n",
    "# SPX 5-minute bar within a +-10 minute window.\n",
    "#\n",
    "# I rename some overlapping columns on both the tweets side and the SPX side to avoid name collisions after the join\n",
    "#\n",
    "# I join tweets and SPX bars on the same calendar day using the date\n",
    "# part of `tweet_time` and `bar_time`. At this stage, each tweet is matched\n",
    "# with *all* SPX bars from that day.\n",
    "#\n",
    "# Step 3: For every tweet–bar pair, I compute the absolute time difference\n",
    "# in seconds between `tweet_time` and `bar_time`, and I only keep those pairs\n",
    "# where the difference is within ±10 minutes (<= 600 seconds). This trims\n",
    "# out bars that are clearly too far from the tweet.\n",
    "#\n",
    "# Step 4: Among the remaining candidates, I use a window partitioned by tweet\n",
    "# `id` and ordered by `time_diff_sec`, then keep only the row with rank 1.\n",
    "# This gives me a single nearest SPX bar for each tweet inside the time window.\n",
    "#\n",
    "# Step 5: I rename `bar_time` to `event_time` for clarity, since this is the\n",
    "# timestamp I’ll treat as the “event bar” in the later modeling. Finally, I\n",
    "# count how many tweets survived this matching step and throw an error if\n",
    "# nothing matched, so I don’t accidentally train on an empty dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# To avoid column name collisions, rename year/hour on both sides\n",
    "tweets_for_join = (\n",
    "    tweets\n",
    "    .withColumnRenamed(\"year\", \"tweet_year\")\n",
    "    .withColumnRenamed(\"hour\", \"tweet_hour\")\n",
    ")\n",
    "\n",
    "spx_for_join = (\n",
    "    spx_rv\n",
    "    .withColumnRenamed(\"year\", \"spx_year\")\n",
    "    .withColumnRenamed(\"hour\", \"spx_hour\")\n",
    ")\n",
    "\n",
    "# 4.1 Join tweets with SPX on the same calendar day\n",
    "joined = (\n",
    "    tweets_for_join\n",
    "    .join(\n",
    "        spx_for_join,\n",
    "        F.to_date(\"tweet_time\") == F.to_date(\"bar_time\"),\n",
    "        \"inner\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4.2 Compute absolute time difference (in seconds) between tweet_time and bar_time\n",
    "joined = joined.withColumn(\n",
    "    \"time_diff_sec\",\n",
    "    F.abs(F.unix_timestamp(\"bar_time\") - F.unix_timestamp(\"tweet_time\"))\n",
    ")\n",
    "\n",
    "# 4.3 Keep only bars within ±10 minutes of the tweet\n",
    "window_secs = 600  # ±10 minutes\n",
    "joined_window = joined.filter(F.col(\"time_diff_sec\") <= window_secs)\n",
    "\n",
    "# 4.4 For each tweet, keep only the single nearest bar\n",
    "w_nearest = Window.partitionBy(\"id\").orderBy(\"time_diff_sec\")\n",
    "\n",
    "events = (\n",
    "    joined_window\n",
    "    .withColumn(\"rn\", F.row_number().over(w_nearest))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\", \"time_diff_sec\")\n",
    ")\n",
    "\n",
    "# Rename bar_time to event_time for clarity\n",
    "events = events.withColumnRenamed(\"bar_time\", \"event_time\")\n",
    "\n",
    "print(\"Events rows (tweets used in pipeline):\", events.count())\n",
    "events.select(\n",
    "    \"id\", \"tweet_time\", \"event_time\",\n",
    "    \"Close\", \"rv_pre_30m\", \"rv_post_30m\",\n",
    "    \"tweet_year\", \"tweet_hour\", \"spx_year\", \"spx_hour\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "if events.count() == 0:\n",
    "    raise ValueError(\"No matched events within ±10 minutes. Check data ranges or window size.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shock threshold (90% quantile of rv_post_30m): 0.0028990257946355995\n",
      "Shock class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|is_shock|count|\n",
      "+--------+-----+\n",
      "|       1| 1087|\n",
      "|       0| 9783|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----+-----------------------------------------+----------------------------------------------------+---------+---------+--------------------------+--------------------+----------------------+---------+--------+--------+----------+---------------------+-------+----------+---+\n",
      "|is_shock|         event_time|year|                                 category|                                       blue_category|sentiment|intensity|has_market_action_keywords|during_trading_hours|towards_ceo_or_company|favorites|retweets|text_len|num_exclam|           rv_pre_30m|  Close|tweet_hour|doy|\n",
      "+--------+-------------------+----+-----------------------------------------+----------------------------------------------------+---------+---------+--------------------------+--------------------+----------------------+---------+--------+--------+----------+---------------------+-------+----------+---+\n",
      "|       0|2018-05-29 10:05:00|2018|     Campaign / Rally / Election Politics|                       Immigration / Border Security| Negative|   Medium|                     False|                True|                 False|    88312|   23260|     277|         1|0.0019230080247938965|2709.29|        10|  3|\n",
      "|       0|2018-05-29 10:30:00|2018|Defense, Military, Sanctions, Geopolitics|            Foreign Policy / Geopolitics / Diplomacy| Positive|   Medium|                     False|                True|                 False|    63540|   13458|     245|         1|0.0012550933708381099|2704.19|        10|  3|\n",
      "|       0|2018-05-29 10:50:00|2018|     Campaign / Rally / Election Politics|Campaign / Elections / Rallies / Political Messaging| Negative|   Medium|                     False|                True|                 False|    64442|   16115|     278|         0| 0.001656133860550514|2705.22|        10|  3|\n",
      "|       0|2018-05-29 11:00:00|2018|     Campaign / Rally / Election Politics|Campaign / Elections / Rallies / Political Messaging| Negative|     High|                     False|                True|                 False|    83557|   18827|     277|         2|0.0019272309432976154|2698.87|        11|  3|\n",
      "|       1|2018-05-29 11:10:00|2018|     Campaign / Rally / Election Politics|Campaign / Elections / Rallies / Political Messaging| Negative|     High|                     False|                True|                 False|    67370|   14586|     230|         2|0.0017407643404526263|2701.13|        11|  3|\n",
      "+--------+-------------------+----+-----------------------------------------+----------------------------------------------------+---------+---------+--------------------------+--------------------+----------------------+---------+--------+--------+----------+---------------------+-------+----------+---+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# PART D. Build shock label (is_shock) and ML dataframe\n",
    "# ==================================================\n",
    "\n",
    "# 5.1 Only consider rows where rv_post_30m exists\n",
    "non_null_events = events.where(F.col(\"rv_post_30m\").isNotNull())\n",
    "if non_null_events.count() == 0:\n",
    "    raise ValueError(\"No non-null rv_post_30m values; cannot build shock label.\")\n",
    "\n",
    "# 5.2 Use 90% quantile of rv_post_30m as shock threshold (more shocks than 95%)\n",
    "shock_quantile = 0.90\n",
    "quantiles = non_null_events.approxQuantile(\"rv_post_30m\", [shock_quantile], 0.0)\n",
    "threshold = float(quantiles[0])\n",
    "print(f\"Shock threshold ({int(shock_quantile*100)}% quantile of rv_post_30m):\", threshold)\n",
    "\n",
    "# 5.3 Define binary label: is_shock = 1 if rv_post_30m > threshold\n",
    "labeled = events.withColumn(\n",
    "    \"is_shock\",\n",
    "    (F.col(\"rv_post_30m\") > F.lit(threshold)).cast(T.IntegerType())\n",
    ")\n",
    "\n",
    "# 5.4 Categorical and numeric feature columns\n",
    "cat_cols = [\n",
    "    \"category\",\n",
    "    \"blue_category\",\n",
    "    \"sentiment\",\n",
    "    \"intensity\",\n",
    "    \"has_market_action_keywords\",\n",
    "    \"during_trading_hours\",\n",
    "    \"towards_ceo_or_company\",\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    \"favorites\",\n",
    "    \"retweets\",\n",
    "    \"text_len\",\n",
    "    \"num_exclam\",\n",
    "    \"rv_pre_30m\",\n",
    "    \"Close\",\n",
    "    \"tweet_hour\",\n",
    "    \"doy\",\n",
    "]\n",
    "\n",
    "# Day-of-week as a simple time feature\n",
    "labeled = labeled.withColumn(\"doy\", F.dayofweek(\"tweet_time\"))\n",
    "\n",
    "# 5.5 Build final ML dataframe\n",
    "ml_df = labeled.select(\n",
    "    \"is_shock\", \"event_time\", \"tweet_year\", *cat_cols, *num_cols\n",
    ").withColumnRenamed(\"tweet_year\", \"year\")\n",
    "\n",
    "print(\"Shock class distribution:\")\n",
    "ml_df.groupBy(\"is_shock\").count().show()\n",
    "\n",
    "ml_df.show(5, truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After NULL cleaning, any NULL in numeric cols?\n",
      "favorites: 0 NULLs\n",
      "retweets: 0 NULLs\n",
      "text_len: 0 NULLs\n",
      "num_exclam: 0 NULLs\n",
      "rv_pre_30m: 0 NULLs\n",
      "Close: 0 NULLs\n",
      "tweet_hour: 0 NULLs\n",
      "doy: 0 NULLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: 3166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 13:03:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GBT sample predictions ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 13:03:27 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----------------------------------------+----------+\n",
      "|event_time         |is_shock|probability                             |prediction|\n",
      "+-------------------+--------+----------------------------------------+----------+\n",
      "|2009-06-30 13:35:00|0       |[0.9044880790851014,0.09551192091489857]|0.0       |\n",
      "|2009-07-28 15:50:00|0       |[0.810774500929421,0.18922549907057895] |0.0       |\n",
      "|2009-08-11 14:50:00|0       |[0.9321156042164586,0.06788439578354144]|0.0       |\n",
      "|2009-08-14 14:35:00|0       |[0.8519157424649585,0.1480842575350415] |0.0       |\n",
      "|2009-09-14 15:50:00|0       |[0.852829543817516,0.14717045618248403] |0.0       |\n",
      "|2009-10-05 14:40:00|0       |[0.7534886013839786,0.24651139861602145]|0.0       |\n",
      "|2009-10-14 14:15:00|0       |[0.7521372489758473,0.24786275102415267]|0.0       |\n",
      "|2010-03-31 13:40:00|0       |[0.9415470637678413,0.05845293623215875]|0.0       |\n",
      "|2010-04-06 14:35:00|0       |[0.9129256092935111,0.08707439070648892]|0.0       |\n",
      "|2010-04-16 15:55:00|0       |[0.7521372489758473,0.24786275102415267]|0.0       |\n",
      "+-------------------+--------+----------------------------------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression sample predictions ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----------------------------------------+----------+\n",
      "|event_time         |is_shock|probability                              |prediction|\n",
      "+-------------------+--------+-----------------------------------------+----------+\n",
      "|2009-06-30 13:35:00|0       |[0.9621662237900255,0.03783377620997452] |0.0       |\n",
      "|2009-07-28 15:50:00|0       |[0.8714417149827358,0.12855828501726418] |0.0       |\n",
      "|2009-08-11 14:50:00|0       |[0.9539380340084452,0.046061965991554765]|0.0       |\n",
      "|2009-08-14 14:35:00|0       |[0.9142697932132346,0.08573020678676535] |0.0       |\n",
      "|2009-09-14 15:50:00|0       |[0.9538459128418048,0.046154087158195245]|0.0       |\n",
      "|2009-10-05 14:40:00|0       |[0.9669801641980873,0.0330198358019127]  |0.0       |\n",
      "|2009-10-14 14:15:00|0       |[0.9560894130978992,0.04391058690210081] |0.0       |\n",
      "|2010-03-31 13:40:00|0       |[0.9779721582785278,0.022027841721472208]|0.0       |\n",
      "|2010-04-06 14:35:00|0       |[0.9603001258033964,0.03969987419660359] |0.0       |\n",
      "|2010-04-16 15:55:00|0       |[0.8986548956668798,0.10134510433312016] |0.0       |\n",
      "+-------------------+--------+-----------------------------------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# PART E. ML pipeline: StringIndexer + OneHotEncoder + GBT\n",
    "# ==================================================\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression\n",
    "\n",
    "# 6.0 Clean NULLs in numeric features to avoid VectorAssembler errors\n",
    "# We simply fill numeric NULLs with 0.0 (you can choose a different strategy if desired).\n",
    "ml_df_clean = ml_df\n",
    "for c in num_cols:\n",
    "    ml_df_clean = ml_df_clean.withColumn(c, F.coalesce(F.col(c), F.lit(0.0)))\n",
    "\n",
    "print(\"After NULL cleaning, any NULL in numeric cols?\")\n",
    "for c in num_cols:\n",
    "    null_cnt = ml_df_clean.filter(F.col(c).isNull()).count()\n",
    "    print(f\"{c}: {null_cnt} NULLs\")\n",
    "\n",
    "# 6.1 Train/test split (random split; could also split by year)\n",
    "train, test = ml_df_clean.randomSplit([0.7, 0.3], seed=42)\n",
    "if test.count() == 0:\n",
    "    test = train\n",
    "\n",
    "print(\"Train rows:\", train.count())\n",
    "print(\"Test rows:\",  test.count())\n",
    "\n",
    "stages_common = []\n",
    "\n",
    "# 6.2 Encode categorical columns (shared by both models)\n",
    "for c in cat_cols:\n",
    "    idx_col = c + \"_idx\"\n",
    "    oh_col = c + \"_oh\"\n",
    "\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=idx_col,\n",
    "        handleInvalid=\"keep\"   # unseen or null categories go to a special bucket\n",
    "    )\n",
    "\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=idx_col,\n",
    "        outputCol=oh_col\n",
    "    )\n",
    "\n",
    "    stages_common += [indexer, encoder]\n",
    "\n",
    "# 6.3 Assemble all features into a single vector\n",
    "feature_cols = [c + \"_oh\" for c in cat_cols] + num_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"   # if any unexpected NULL sneaks in, keep instead of error\n",
    ")\n",
    "stages_common.append(assembler)\n",
    "\n",
    "# ==================================================\n",
    "# 6.4 Model 1: Gradient-Boosted Trees (GBT)\n",
    "# ==================================================\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"is_shock\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=3,\n",
    "    maxIter=20,\n",
    ")\n",
    "\n",
    "gbt_pipeline = Pipeline(stages=stages_common + [gbt])\n",
    "\n",
    "gbt_model = gbt_pipeline.fit(train)\n",
    "gbt_pred = gbt_model.transform(test)\n",
    "\n",
    "print(\"=== GBT sample predictions ===\")\n",
    "gbt_pred.select(\"event_time\", \"is_shock\", \"probability\", \"prediction\") \\\n",
    "        .show(10, truncate=False)\n",
    "\n",
    "# ==================================================\n",
    "# 6.5 Model 2: Logistic Regression (baseline linear model)\n",
    "# ==================================================\n",
    "\n",
    "# We can reuse the same preprocessing stages (indexer + OHE + assembler),\n",
    "# but need a fresh pipeline object to attach LogisticRegression at the end.\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"is_shock\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=50,\n",
    "    regParam=0.0,      # you can tune regularization if needed\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(stages=stages_common + [lr])\n",
    "\n",
    "lr_model = lr_pipeline.fit(train)\n",
    "lr_pred = lr_model.transform(test)\n",
    "\n",
    "print(\"=== Logistic Regression sample predictions ===\")\n",
    "lr_pred.select(\"event_time\", \"is_shock\", \"probability\", \"prediction\") \\\n",
    "       .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GBT] PR-AUC:  0.7138\n",
      "[GBT] ROC-AUC: 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR ] PR-AUC:  0.6498\n",
      "[LR ] ROC-AUC: 0.9066\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# PART F. Evaluation metrics (PR-AUC and ROC-AUC)\n",
    "# ==================================================\n",
    "\n",
    "if test.select(\"is_shock\").distinct().count() > 1:\n",
    "    evaluator_pr = BinaryClassificationEvaluator(\n",
    "        labelCol=\"is_shock\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderPR\"\n",
    "    )\n",
    "    evaluator_roc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"is_shock\",\n",
    "        rawPredictionCol=\"rawPrediction\",\n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "\n",
    "    # Evaluate GBT\n",
    "    gbt_pr_auc = evaluator_pr.evaluate(gbt_pred)\n",
    "    gbt_roc_auc = evaluator_roc.evaluate(gbt_pred)\n",
    "    print(f\"[GBT] PR-AUC:  {gbt_pr_auc:.4f}\")\n",
    "    print(f\"[GBT] ROC-AUC: {gbt_roc_auc:.4f}\")\n",
    "\n",
    "    # Evaluate Logistic Regression\n",
    "    lr_pr_auc = evaluator_pr.evaluate(lr_pred)\n",
    "    lr_roc_auc = evaluator_roc.evaluate(lr_pred)\n",
    "    print(f\"[LR ] PR-AUC:  {lr_pr_auc:.4f}\")\n",
    "    print(f\"[LR ] ROC-AUC: {lr_roc_auc:.4f}\")\n",
    "else:\n",
    "    print(\"Test set has only one class label; PR/ROC evaluation is not meaningful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT model saved to: /storage/work/yfl5682/Project/models/gbt_model\n",
      "Logistic Regression model saved to: /storage/work/yfl5682/Project/models/lr_model\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Save models\n",
    "# ==================================================\n",
    "\n",
    "model_dir = \"/storage/work/yfl5682/Project/models\"\n",
    "\n",
    "# Save GBT model\n",
    "gbt_save_path = f\"{model_dir}/gbt_model\"\n",
    "gbt_model.write().overwrite().save(gbt_save_path)\n",
    "print(f\"GBT model saved to: {gbt_save_path}\")\n",
    "\n",
    "# Save Logistic Regression model\n",
    "lr_save_path  = f\"{model_dir}/lr_model\"\n",
    "lr_model.write().overwrite().save(lr_save_path)\n",
    "print(f\"Logistic Regression model saved to: {lr_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f25)",
   "language": "python",
   "name": "ds410_f25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
